# 深度学习

### 1. 以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播

<img src="imgs/deep_learning/relu_backward.jpg" style="zoom:20%;" />

### 2. NN的权重参数能否初始化为0？

不能，可能导致模型无法收敛。



### 7. 一个隐层需要多少节点能实现包含n元输入的任意布尔函数？

>   布尔函数是从布尔域{0,1}到{0,1}的映射，它的输出只能是0或1。实现一个包含n元输入的任意布尔函数可以看做是在输入的所有取值组合上对输出进行分类，因此可以使用一个二元决策树来实现任意布尔函数。对于n个输入变量，决策树的最大深度为n，所以决策树最多只需要2^n个节点即可满足任意布尔函数的实现。
>   然而，决策树实现布尔函数存在一个问题，它容易出现过拟合。因此，研究人员提出了各种替代方案，如神经网络。一个单层的感知器可以实现基本的布尔函数，但对于更加复杂的布尔函数，我们需要多层感知器来实现。下面我们来讨论一个隐层需要多少节点可以实现n元输入的任意布尔函数。
>   已知一个隐层神经网络可以表示的函数类为
>   $$
>   f(x)=\sigma(\sum_{i=1}^{m}w_{i}\sigma(\sum_{j=1}^{n}u_{ij}x_{j}+b_{i})+b)
>   $$
>   其中，$\sigma$为一个非线性激活函数，$m$和$n$分别为隐层和输入层的节点数，$w_{i}$和$u_{ij}$为权重，$b_{i}$和$b$为偏置。
>   可以证明，一个具备足够数量的隐层节点的神经网络，可以在有限训练次数内近似任意布尔函数。更准确地说，对于一个任意布尔函数 $g: {0,1}^n \rightarrow {0,1}$，存在一个具备 $O(2^n)$ 个隐层节点的神经网络可以在训练次数内关于训练数据$D$的近似误差 $\epsilon$达到误差下限。
>   因此，一个隐层需要 $O(2^n)$ 个节点来实现包含n元输入的任意布尔函数。然而，为了减少过拟合和提高训练效率，我们不必总是使用最小数量的节点。实践中，往往需要进行一些实验才能确定网络的节点数。

### 8. 多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？

> 问题7 和 8 参考：https://zhuanlan.zhihu.com/p/32579088

### 10. CNN和FCN的根本不同之处在哪？

CNN: 图像级的分类

经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）

FCN: 像素级的分类

采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类

### 11. Triplet Loss 怎么生成那三个点

> https://zhuanlan.zhihu.com/p/171627918

### 12. LSTM解决了RNN的什么问题，怎么解决的？

>   LSTM（长短期记忆网络）是一种循环神经网络（RNN）的变体，能够解决传统RNN中存在的梯度消失和梯度爆炸的问题，从而能够更好地应用于对长序列数据的建模。
>   在传统的RNN中，输入数据的信息会沿时间步传播，但是当输入序列长度很长时，RNN容易遇到梯度消失和梯度爆炸的问题，导致网络无法很好地捕捉长期依赖关系。LSTM通过引入三个门（输入门、遗忘门和输出门）来解决这个问题，并能够在处理时间序列数据时更加高效且准确。
>   LSTM的三个门控制着信息的流动，以使得LSTM能够选择遗忘或保留当前的输入。门的计算会考虑当前输入、前一个时间步的LSTM状态和前一个时间步的输出，从而可以过滤掉与当前序列无关的信息。LSTM通过这种方法显式地管理状态信息的流动，使得其在处理长序列时更加鲁棒。
>   具体来说，LSTM通过引入一个称为细胞状态（cell state）的中间状态，并通过输入门和遗忘门控制细胞状态的输入和遗忘，从而解决了梯度消失和爆炸问题。LSTM还通过输出门调整输出，以确保只输出与当前输入相关的信息，从而实现精确的序列建模。
>   总的来说，LSTM解决了传统RNN中存在的梯度消失和梯度爆炸问题，通过引入门控制思想以及细胞状态，使得其在处理长序列数据时更加准确和高效。此外，LSTM也广泛用于各种自然语言处理任务，如文本分类、情感分析等。

### 13. LSTM的变种，以及BPTT(基于时间的反向传播)

### 14. CNN的优点、缺点、参数计算

### 15. VGG16提出了哪几个改进点？3*3卷积和1*1卷积的作用

### 16. inception结构有什么特点

### 17. 对NLP的理解

### 18.什么是wide&deep？

### 19. 当你选择模型你考虑哪些因素？

1. 模型的性能
2. 结果的可解释性
3. 模型的复杂性
4. 数据集的大小
5. 数据的维度
6. 训练时间和成本
7. 推理时间

### 20. BN的gama，labada的作用

> https://zhuanlan.zhihu.com/p/93643523

缩放和平移 

### 21.Adam优化器，bn的作用，以及为什么能加快收敛速度？

Adam优化器的优点

1. 实现简单，计算高效，对内存需求少
2. 参数的更新不受梯度的伸缩变换影响
3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调
4. 更新的步长能够被限制在大致的范围内（初始学习率）
5. 能自然地实现步长退火过程（自动调整学习率）
6. 很适合应用于大规模的数据及参数的场景
7. 适用于不稳定目标函数
8. 适用于梯度稀疏或梯度存在很大噪声的问题

### 22. 多模态

### 23. TFIDF

### 24. pointnet

### 25. attention model







