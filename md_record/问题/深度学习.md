# 深度学习

### 1. 以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播

<img src="imgs/deep_learning/relu_backward.jpg" style="zoom:20%;" />

### 2. NN的权重参数能否初始化为0？

不能，可能导致模型无法收敛。

### 3. 什么是梯度消失和梯度爆炸？

> 参考：https://cloud.tencent.com/developer/article/1374163

* 梯度消失

  > 根据链式法则，如果每一层神经元对上一层神经元的输出的偏导乘上权重结果都小于1的话，即使这个结果是0.99，在经过多层传播后，误差对输入层的偏导会趋于0
  >
  > 这种情况会导致靠近输入层的隐藏层神经元调整极小

* 梯度爆炸

  > 根据链式法则，如果每一层神经元对上一层神经元的输出的偏导乘上权重结果都大于1的话，在经过多层传播后，误差对输入层的偏导会趋于无穷大
  >
  > 这种情况会导致靠近输入层的隐藏层神经元调整极大

* 解决方案

  * 预训练加微调

  > 此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。
  >
  > Hinton在训练深度信念网络（Deep Belief Networks）中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

  * 梯度剪切、正则(梯度爆炸)

  > 其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内，通过这种直接的方法就可以防止梯度爆炸。

  > 权重正则化（weithts regularization）比较常见的是l1正则，和l2正则
  >
  > 正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：
  >
  > ![](imgs/weight_regularity_formula.png)
  >
  > 其中，α 是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

  * ##### **relu、leakrelu、elu等激活函数**

  > 思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。
  >
  > Relu的主要贡献在于：
  >
  > * 解决了梯度消失、爆炸问题
  > * 计算方便、计算速度快
  > * 加速了网络的训练
  >
  > 缺点：
  >
  > * 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
  > * 输出不是以0为中心的

  * ##### **Batchnorm**

  > Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。
  >
  > 通过规范化操作将输出x规范化以此来保证网络的稳定性。
  >
  > batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。
  >
  > 
  >
  > 参考：https://blog.csdn.net/qq_25737169/article/details/79048516

  * **残差结构**
  * **LSTM**

### 4. sigmoid和tanh为什么会导致梯度消失？

* sigmoid函数出现梯度消失的原因

> 当神经元的激活在接近0或者1处时，梯度几乎为0
>
> 在反向传播时，这个（局部）梯度将会与整个损失函数关于改门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会非常小，这会有效地“杀死”梯度，几乎就没有信号通过神经元传到权重再到数据了。
>
> 为了防止饱和，在权重初始化时也需特别留意。如果过大，将会导致神经元饱和，不再学习了

* tanh函数出现梯度消失的原因

> 函数tanh和函数sigmoid一样，在其饱和区的接近于0，都容易产生后续梯度消失、计算量大的问题



### 5. dropout为何能防止过拟合？

> https://zhuanlan.zhihu.com/p/23178423

> 防止参数过分依赖训练数据，增加参数对数据集的泛化能力
>
> > 假设要训练这样一个网络：
> >
> > ![](imgs/example2.png)
> >
> > 正常的流程是：首先把x通过网络前向传播；然后把误差反向传播以决定 如何更新参数 让网络进行学习。
> >
> > 使用dropout之后过程变成：
> >
> > ![](imgs/example3.png)
> >
> > 1. 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（下图中虚线为部分临时被删除的神经元）
> > 2. 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后就按照随机梯度下降法更新（没有被删除的神经元）对应的参数（w，b）。
> > 3. 然后继续重复这一过程：
> >    - 恢复被删掉的神经元（此时 被删除的神经元 保持原样，而没有被删除的神经元已经有所更新）
> >    - 从隐藏神经元中随机选择一个一半大小的子集 临时删除掉（备份被删除神经元的参数）。
> >    - 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）
> > 4. 重复这一过程
>
> 为什么能防止过拟合呢？
>
> * **取平均的作用**： 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。**因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消**。dropout掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，**整个dropout过程就相当于 对很多个不同的神经网络取平均**。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
> * **减少神经元之间复杂的共适应关系**： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）

### 6. dropout和BN 在前向传播和方向传播阶段的区别？

### 7. 一个隐层需要多少节点能实现包含n元输入的任意布尔函数？

### 8. 多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？

> 问题7 和 8 参考：https://zhuanlan.zhihu.com/p/32579088

### 9. 感受野

>   是指输出特征图上某个像素对应到输入空间中的区域范围，可以理解为特征图像素到输入区域的映射。

### 10. CNN和FCN的根本不同之处在哪？

### 11. Triplet Loss 怎么生成那三个点

### 12. LSTM解决了RNN的什么问题，怎么解决的？

### 13. LSTM的变种，以及BPTT(基于时间的反向传播)

### 14. CNN的优点、缺点、参数计算

### 15. VGG16提出了哪几个改进点？3*3卷积和1*1卷积的作用

### 16. inception结构有什么特点

### 17. 对NLP的理解

### 18.什么是wide&deep？

### 19. 当你选择模型你考虑哪些因素？

### 20. BN的gama，labada的作用

缩放和平移 

### 21.Adam优化器，bn的作用，以及为什么能加快收敛速度？

### 22. 多模态

### 23. TFIDF

### 24. pointnet

### 25. attention model

### 26. BN的原理、作用；手动推导

>   参考：[(3条消息) BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结_夏洛的网的博客-CSDN博客](https://blog.csdn.net/liuxiao214/article/details/81037416)
>
>   [深度学习中的五种归一化（BN、LN、IN、GN和SN）方法简介_in归一化_时光碎了天的博客-CSDN博客](https://blog.csdn.net/u013289254/article/details/99690730)

#### 原理

#### 作用

*   加快网络的训练速度与收敛的速度
*   控制梯度爆炸防止梯度消失
*   防止过拟合



## CNN

### 1. 空洞卷积是什么？有什么应用场景？

>   -   空洞卷积就是在卷积的时候不是对连续的值进行计算，而是会中间隔开几个值，其他与普通的卷积并无差异。可以理解为一个稀疏的大卷积核，只有个别值有效，其余值为零。

* 应用

> 01.在卷积图上注入空洞，增加感受野。注入空洞的数量由dilation rate确定。常规卷积的dilation rate为1。
>
> 02.多尺度检测，利于检测出小物体
>
> 03.语义分割中常用dilation rate。但是人像分割中无用，应该就是我们的应用场景没有特别小的物体。

### 2. resnet提出的背景和核心理论是？

* 背景

> 当模型深度增加到某个程度后，在增加深度，模型效果可能不升反降，出现退化现象。（不是过拟合也不是梯度爆炸或消失）

* 核心理论

> 恒等映射

### 3. CNN如何用于文本分类？



### 4. 常用的池化操作有哪些？有什么特点？

> https://zhuanlan.zhihu.com/p/112216409

* 优点

  > 1. 抑制噪声，降低信息冗余
  > 2. 提升模型的尺度不变性、旋转不变性
  > 3. 降低模型计算量
  > 4. 防止过拟合

* 最大/平均池化

  * 最大池化

  > 最大池化就是选择图像区域中最大值作为该区域池化以后的值，反向传播的时候，梯度通过前向传播过程的最大值反向传播，其他位置梯度为0。
  >
  > ```python
  > import torch
  > import torch.nn.functional as F
  > input_ = torch.Tensor(4,3,16,16)
  > output = F.max_pool2d(input_, kernel_size=2, stride=2)
  > print(output.shape)
  > ```
  >
  > 

  * 平均池化

  > 将选择的图像区域中的平均值作为该区域池化以后的值。
  >
  > ```python
  > import torch
  > import torch.nn.functional as F
  > input_ = torch.Tensor(4,3,16,16)
  > output = F.avg_pool2d(input_, kernel_size=2, stride=2)
  > print(output.shape)
  > ```

* 随机池化

  > 特征区域的大小越大，代表其被选择的概率越高

* 中值池化

* 组合池化

  > 组合池化则是同时利用最大值池化与均值池化两种的优势而引申的一种池化策略。
  >
  > 常见组合策略有两种：Cat与Add。
  >
  > 常常被当做分类任务的一个trick，其作用就是丰富特征层，maxpool更关注重要的局部特征，而average pooling更关注全局特征。
  >
  > ```python
  > import torch.nn.functional as F
  > 
  > def add_avgmax_pool2d(x, output_size=1):
  >      x_avg = F.adaptive_avg_pool2d(x, output_size)
  >      x_max = F.adaptive_max_pool2d(x, output_size)
  >      return 0.5 * (x_avg + x_max)
  > 
  > def cat_avgmax_pool2d(x, output_size=1):
  >      x_avg = F.adaptive_avg_pool2d(x, output_size)
  >      x_max = F.adaptive_max_pool2d(x, output_size)
  >      return torch.cat([x_avg, x_max], 1)
  > ```

* Spatial Pyramid Pooling

  > SPP是在SPPNet中提出的，SPPNet提出比较早，在RCNN之后提出的，用于解决重复卷积计算和固定输出的两个问题，具体方法如下图所示：
  >
  > ![](imgs/deep_learning/pooling/SPP_pooling.webp)
  >
  > 在feature map上通过selective search获得窗口，然后将这些区域输入到CNN中，然后进行分类。
  >
  > 实际上SPP就是多个空间池化的组合，对不同输出尺度采用不同的划窗大小和步长以确保输出尺度相同，同时能够融合金字塔提取出的多种尺度特征，能够提取更丰富的语义信息。常用于多尺度训练和目标检测中的RPN网络。
  >
  > * SPP有效地原因
  >
  >   > 1. 从感受野角度来讲，之前计算感受野的时候可以明显发现，maxpool的操作对感受野的影响非常大，其中主要取决于kernel size大小。在SPP中，使用了kernel size非常大的maxpool会极大提高模型的感受野，笔者没有详细计算过darknet53这个backbone的感受野，在COCO上有效很可能是因为backbone的感受野还不够大。
  >   >
  >   > 2. 第二个角度是从Attention的角度考虑，这一点启发自CSDN@小楞（链接在参考文献中），他在文章中这样讲：
  >   >
  >   >    > 出现检测效果提升的原因：通过spp模块实现局部特征和全局特征（所以空间金字塔池化结构的最大的池化核要尽可能的接近等于需要池化的featherMap的大小）的featherMap级别的融合，丰富最终特征图的表达能力，从而提高MAP。
  >   >    >
  >   >    > https://blog.csdn.net/qq_33270279/article/details/103898245
  >
  > 

* Global Average/Max Pooling

  >Gloabel Average Pooling 是NIN里边的做法，一般使用torchvision提供的预训练模型进行finetune的时候，通常使用Global Average Pooling，原因就是可以不考虑图片的输入尺寸，只与filter有关。
  >
  >```python
  >import torch
  >from torch.nn import AdaptiveAvgPool2d
  >input = torch.zeros((4,12,18,18)) # batch size, fileter, h, w
  >gap = AdaptiveAvgPool2d(1)
  >output = gap(input)
  >print(output.shape)
  >print(output.view(input.shape[0],-1).shape)
  >```

* NetVLAD池化

* 双线性池化

  > Bilinear Pooling是在《Bilinear CNN Models for Fine-grained Visual Recognition》被提出的，主要用在细粒度分类网络中。双线性池化主要用于特征融合，对于同一个样本提取得到的特征x和特征y, 通过双线性池化来融合两个特征(外积)，进而提高模型分类的能力。
  >
  > > 主要思想是对于两个不同图像特征的处理方式上的不同。传统的，对于图像的不同特征，我们常用的方法是进行串联（连接），或者进行sum,或者max-pooling。论文的主要思想是，研究发现人类的大脑发现，人类的视觉处理主要有两个pathway, the ventral stream是进行物体识别的，the dorsal stream 是为了发现物体的位置。
  > > 论文基于这样的思想，希望能够将两个不同特征进行结合来共同发挥作用，提高细粒度图像的分类效果。论文希望两个特征能分别表示图像的位置和对图形进行识别。论文提出了一种Bilinear Model。
  >
  > 如果特征 x 和特征y来自两个特征提取器，则被称为**多模双线性池化**（MBP，Multimodal Bilinear Pooling）
  >
  > 如果特征 x = 特征 y，则被称为**同源双线性池化**（HBP，Homogeneous Bilinear Pooling）或者二阶池化（Second-order Pooling）。
  >
  > ![](imgs/deep_learning/pooling/Double_mode_linear_pooling.webp)
  >
  > ```python
  > import torch
  > X = torch.reshape(N, D, H * W)                        # Assume X has shape N*D*H*W
  > X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling
  > assert X.size() == (N, D, D)
  > X = torch.reshape(X, (N, D * D))
  > X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization
  > X = torch.nn.functional.normalize(X)                  # L2 normalization
  > ```
  >
  > 参考： https://zhuanlan.zhihu.com/p/62532887

* UnPooling

  > 是一种上采样操作
  >
  > ![](imgs/deep_learning/pooling/UnPooling.webp)
  >
  > 流程描述：
  >
  > > 1. 在Pooling（一般是Max Pooling）时，保存最大值的位置。
  > > 2. 中间经历若干网络层的运算。
  > > 3. 上采样阶段，利用第1步保存的Max Location，重建下一层的feature map。
  >
  > **UnPooling不完全是Pooling的逆运算**，Pooling之后的feature map，要经过若干运算，才会进行UnPooling操作；对于非Max Location的地方以零填充。然而这样并不能完全还原信息。

### 5. 共享参数有什么优点

* 削减参数量，压缩模型复杂度
* 实现平移不变性

### 6. 网络容量计算方法

参考经典网络结构文档

### 7. 给定卷积核的尺寸，特征图大小计算方法？

![](imgs/example6.png)

