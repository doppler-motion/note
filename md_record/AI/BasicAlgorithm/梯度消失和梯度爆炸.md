### 3. 什么是梯度消失和梯度爆炸？

> 参考：https://cloud.tencent.com/developer/article/1374163

* 梯度消失

    > 根据链式法则，如果每一层神经元对上一层神经元的输出的偏导乘上权重结果都小于1的话，即使这个结果是0.99，在经过多层传播后，误差对输入层的偏导会趋于0
    >
    > 这种情况会导致靠近输入层的隐藏层神经元调整极小

* 梯度爆炸

    > 根据链式法则，如果每一层神经元对上一层神经元的输出的偏导乘上权重结果都大于1的话，在经过多层传播后，误差对输入层的偏导会趋于无穷大
    >
    > 这种情况会导致靠近输入层的隐藏层神经元调整极大

* 解决方案

    * 预训练加微调

    > 此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。
    >
    > Hinton在训练深度信念网络（Deep Belief Networks）中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

    * 梯度剪切、正则(梯度爆炸)

    > 其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内，通过这种直接的方法就可以防止梯度爆炸。

    > 权重正则化（weithts regularization）比较常见的是l1正则，和l2正则
    >
    > 正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：
    > $$
    > Loss = (y - W^{T}x)^{2} + \alpha||W||^{2}
    > $$
    >
    >
    > 其中，$\alpha$是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

    * ##### **relu、leakrelu、elu等激活函数**

    > 思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。
    >
    > Relu的主要贡献在于：
    >
    > * 解决了梯度消失、爆炸问题
    > * 计算方便、计算速度快
    > * 加速了网络的训练
    >
    > 缺点：
    >
    > * 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
    > * 输出不是以0为中心的

    * ##### **Batchnorm**

    > Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。
    >
    > 通过规范化操作将输出x规范化以此来保证网络的稳定性。
    >
    > batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。
    >
    > 
    >
    > 参考：https://blog.csdn.net/qq_25737169/article/details/79048516

    * **残差结构**
    * **LSTM**

### 4. sigmoid和tanh为什么会导致梯度消失？

* sigmoid函数出现梯度消失的原因

> 当神经元的激活在接近0或者1处时，梯度几乎为0
>
> 在反向传播时，这个（局部）梯度将会与整个损失函数关于改门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会非常小，这会有效地“杀死”梯度，几乎就没有信号通过神经元传到权重再到数据了。
>
> 为了防止饱和，在权重初始化时也需特别留意。如果过大，将会导致神经元饱和，不再学习了

* tanh函数出现梯度消失的原因

> 函数tanh和函数sigmoid一样，在其饱和区的接近于0，都容易产生后续梯度消失、计算量大的问题