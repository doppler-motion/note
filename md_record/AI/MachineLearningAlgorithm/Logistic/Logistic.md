# Logistic

**介绍一下逻辑回归**

逻辑回归（logistic regression）是一种二分类算法，它以线性回归为基础，在其输出结果上套用了逻辑函数（sigmoid）将结果映射到[0,1]区间内，并将其解释为概率值。因此逻辑回归可以用于处理概率问题，并据此作出二分类决策。

逻辑回归通过学习已标记或类别已知的训练样本，构建一个逻辑回归模型，然后将该模型用于对新样本进行预测。逻辑回归的优点是实现起来简单，经常用于分类问题；同时，逻辑回归模型的系数还可以解释为不同特征对结果的贡献度。

逻辑回归的模型可以表示为：
$$
P(y=1|x)=\dfrac{1}{1+exp(-w^Tx-b)}
$$




其中，$x$ 是输入变量，$w$ 是输入变量的权重， $b$ 是偏差项，$y$ 代表类别输出，$p(y=1|x)$ 是输出类别为正例（正样本）的概率。

在训练阶段，逻辑回归模型最常采用的是最大似然估计（Maximum Likelihood Estimation，MLE）来估计模型参数 $w$ 和 $b$。在预测阶段，需要对新数据进行输入，进行预测时将输入数据带入模型中，再根据模型输出的概率阈值来进行分类。

需要注意的是，逻辑回归模型仅适用于线性可分的数据。如果数据呈现出复杂的非线性关系，则需要结合其他的技术，如核方法或非线性转换，来处理该问题。另外，应注意选择适合的优化算法和正则化方法，以避免过拟合以及其他问题的发生。



**逻辑回归使用的损失函数**

逻辑回归是一种二分类问题的常见算法，其损失函数通常采用交叉熵损失函数（Cross-Entropy Loss Function）。其具体形式如下：

$L(w,b) = -\frac{1}{n}\sum_{i=1}^n \left(y^{(i)}\log\left(\hat{y}^{(i)}\right) + (1-y^{(i)})\log\left(1-\hat{y}^{(i)}\right) \right)$

其中，$y^{(i)}$表示训练数据的真实标签（0或1），$\hat{y}^{(i)}$表示模型对第i个样本的预测标签。$w$和$b$表示模型的参数。

交叉熵损失函数的核心思想是通过比较模型输出的概率分布与真实标签的概率分布（one-hot表示），来计算预测值与真实值之间的差异。当模型对某个样本的预测结果与真实结果相符时，损失函数的值最小。当模型的预测结果与真实结果不同时，损失函数的值会逐渐增大，以反映预测错误的程度。

交叉熵损失函数具有很好的数学性质，在梯度下降中易于求导和优化。它还可以避免因特征量纲问题、非线性问题等造成的误差放大等问题。因此，交叉熵损失函数是逻辑回归中常用的损失函数。

